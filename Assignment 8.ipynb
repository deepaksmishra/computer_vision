{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa82655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Using our own terms and diagrams, explain INCEPTIONNET ARCHITECTURE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a582d336",
   "metadata": {},
   "source": [
    "The paper proposes a new type of architecture – GoogLeNet or Inception v1. It is basically a convolutional neural network (CNN) which is 27 layers deep. ... 1×1 Convolutional layer before applying another layer, which is mainly used for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659e876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Describe the Inception block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398ce02c",
   "metadata": {},
   "source": [
    "An Inception Module is an image model block that aims to approximate an optimal local sparse structure in a CNN. Put simply, it allows for us to use multiple types of filter size, instead of being restricted to a single filter size, in a single image block, which we then concatenate and pass onto the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1b5202",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. What is the DIMENSIONALITY REDUCTION LAYER (1 LAYER CONVOLUTIONAL)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d816486",
   "metadata": {},
   "source": [
    "To address this problem, a 1×1 convolutional layer can be used that offers a channel-wise pooling, often called feature map pooling or a projection layer. This simple technique can be used for dimensionality reduction, decreasing the number of feature maps whilst retaining their salient features. It can also be used directly to create a one-to-one projection of the feature maps to pool features across channels or to increase the number of feature maps, such as after traditional pooling layers.\n",
    "\n",
    "In this tutorial, you will discover how to use 1×1 filters to control the number of feature maps in a convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d99a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. THE IMPACT OF REDUCING DIMENSIONALITY ON NETWORK PERFORMANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b6a97c",
   "metadata": {},
   "source": [
    "It reduces the time and storage space required. It helps Remove multi-collinearity which improves the interpretation of the parameters of the machine learning model. It becomes easier to visualize the data when reduced to very low dimensions such as 2D or 3D. It avoids the curse of dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2953d488",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Mention three components. Style GoogLeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9448f7e7",
   "metadata": {},
   "source": [
    "The GoogLeNet architecture is very different from previous state-of-the-art architectures such as AlexNet and ZF-Net. It uses many different kinds of methods such as 1×1 convolution and global average pooling that enables it to create deeper architecture. In the architecture, we will discuss some of these methods\n",
    "he winner of the ILSVRC 2014 competition was GoogLeNet(a.k.a. Inception V1) from Google. It achieved a top-5 error rate of 6.67%! This was very close to human level performance which the organisers of the challenge were now forced to evaluate. As it turns out, this was actually rather hard to do and required some human training in order to beat GoogLeNets accuracy. After a few days of training, the human expert (Andrej Karpathy) was able to achieve a top-5 error rate of 5.1%(single model) and 3.6%(ensemble)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f4e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Using our own terms and diagrams, explain RESNET ARCHITECTURE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc12354a",
   "metadata": {},
   "source": [
    "ResNet, short for Residual Network is a specific type of neural network that was introduced in 2015 by Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun in their paper “Deep Residual Learning for Image Recognition”.The ResNet models were extremely successful which you can guess from the following:\n",
    "\n",
    "Won 1st place in the ILSVRC 2015 classification competition with a top-5 error rate of 3.57% (An ensemble model)\n",
    "Won the 1st place in ILSVRC and COCO 2015 competition in ImageNet Detection, ImageNet localization, Coco detection and Coco segmentation.\n",
    "Replacing VGG-16 layers in Faster R-CNN with ResNet-101. They observed relative improvements of 28%\n",
    "Efficiently trained networks with 100 layers and 1000 layers also.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031c5838",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. What do Skip Connections entail?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623a4cf3",
   "metadata": {},
   "source": [
    "Skip Connections (or Shortcut Connections) as the name suggests skips some of the layers in the neural network and feeds the output of one layer as the input to the next layers. Skip Connections were introduced to solve different problems in different architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8761af",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. What is the definition of a residual Block?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5338049f",
   "metadata": {},
   "source": [
    "A residual block is simply when the activation of a layer is fast-forwarded to a deeper layer in the neural network. Example of a residual block. As you can see in the image above, the activation from a previous layer is being added to the activation of a deeper layer in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4523a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. How can transfer learning help with problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ce99fd",
   "metadata": {},
   "source": [
    "Transfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab3082",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. What is transfer learning, and how does it work?\n",
    "HOW DO NEURAL NETWORKS LEARN FEATURES? 11. HOW DO NEURAL NETWORKS LEARN\n",
    "FEATURES?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdc1af7",
   "metadata": {},
   "source": [
    "The reuse of a pre-trained model on a new problem is known as transfer learning in machine learning. A machine uses the knowledge learned from a prior assignment to increase prediction about a new task in transfer learning. You could, for example, use the information gained during training to distinguish beverages when training a classifier to predict whether an image contains cuisine.\n",
    "\n",
    "The knowledge of an already trained machine learning model is transferred to a different but closely linked problem throughout transfer learning. For example, if you trained a simple classifier to predict whether an image contains a backpack, you could use the model’s training knowledge to identify other objects such as sunglasses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47143c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "12. WHY IS FINE-TUNING BETTER THAN START-UP TRAINING?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe1f4b2",
   "metadata": {},
   "source": [
    "In the Flicker-style example the situation is a bit more generic. They use the weights of first layers from a model trained for a different classification task and employ it for a new task, training only a new last layer and fine-tuning the first layers a bit (by setting a low learning rate for those pretrained layers). Your case is similar but more specific, you want to use the pretrained model to train the exact architecture for the exact same task but with an extension of your data.\n",
    "\n",
    "If your question if whether Option 1. will produce exactly the same model (all resulting weights are equal) as Option 2. Then no, most probably not.\n",
    "\n",
    "In Option 2. the network is trained for iterations of dataset A then for dataset B then dataset A again..and so on (assuming both were just concatenated together). While in Option 1. will have the network trained for some iterations/epochs on dataset A, then later continue learning for iterations/epochs on only dataset B and that's it. So the solver will see a different sequence of gradients in both options resulting in two different models. That's from a strict theoretical perspective.\n",
    "\n",
    "If you ask from a practical perspective, the two options will probably end up with very similar models. How many epochs (not iterations) did you train on dataset A ? say N epochs, then you can safely go with Option 2. and train your existing model further on dataset B for the same number of epochs and same learning rate and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f748b925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87842921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
