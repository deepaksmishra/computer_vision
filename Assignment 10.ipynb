{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c27d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Why don&#39;t we start all of the weights with zeros?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0434f73a",
   "metadata": {},
   "source": [
    "When there is no change in the Output, there is no gradient and hence no direction. Main problem with initialization of all weights to zero mathematically leads to either the neuron values are zero (for multi layers) or the delta would be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9438d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Why is it beneficial to start weights with a mean zero distribution?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26deba3",
   "metadata": {},
   "source": [
    "Initializing all the weights with zeros leads the neurons to learn the same features during training. ... Thus, both neurons will evolve symmetrically throughout training, effectively preventing different neurons from learning different things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f9c735",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. What is dilated convolution, and how does it work?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3834ba73",
   "metadata": {},
   "source": [
    "In simple terms, dilated convolution is just a convolution applied to input with defined gaps. With this definitions, given our input is an 2D image, dilation rate k=1 is normal convolution and k=2 means skipping one pixel per input and k=4 means skipping 3 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5423a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. What is TRANSPOSED CONVOLUTION, and how does it work?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77ad228",
   "metadata": {},
   "source": [
    "Transposed convolutions are standard convolutions but with a modified input feature map. The stride and padding do not correspond to the number of zeros added around the image and the amount of shift in the kernel when sliding it across the input, as they would in a standard convolution operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0fe0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "5.Explain Separable convolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec975512",
   "metadata": {},
   "source": [
    "The spatial separable convolution is so named because it deals primarily with the spatial dimensions of an image and kernel: the width and the height. (The other dimension, the “depth” dimension, is the number of channels of each image). A spatial separable convolution simply divides a kernel into two, smaller kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a414fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "6.What is depthwise convolution, and how does it work?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d3299f",
   "metadata": {},
   "source": [
    "Depthwise Convolution is a type of convolution where we apply a single convolutional filter for each input channel. In the regular 2D convolution performed over multiple input channels, the filter is as deep as the input and lets us freely mix channels to generate each element in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0a8d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "7.What is Depthwise separable convolution, and how does it work?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf05aad8",
   "metadata": {},
   "source": [
    "Unlike spatial separable convolutions, depthwise separable convolutions work with kernels that cannot be “factored” into two smaller kernels. Hence, it is more commonly used. This is the type of separable convolution seen in keras.layers.SeparableConv2D or tf.layers.separable_conv2d.\n",
    "The depthwise separable convolution is so named because it deals not just with the spatial dimensions, but with the depth dimension — the number of channels — as well. An input image may have 3 channels: RGB. After a few convolutions, an image may have multiple channels. You can image each channel as a particular interpretation of that image; in for example, the “red” channel interprets the “redness” of each pixel, the “blue” channel interprets the “blueness” of each pixel, and the “green” channel interprets the “greenness” of each pixel. An image with 64 channels has 64 different interpretations of that image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0afbc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "8.Capsule networks are what they sound like.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96c2990",
   "metadata": {},
   "source": [
    "\n",
    "Capsule networks (CapsNets), a new class of deep neural network architectures proposed recently by Hinton et al., have shown a great performance in many fields, particularly in image recognition and natural language processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abed701",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Why is POOLING such an important operation in CNNs?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e5e619",
   "metadata": {},
   "source": [
    "Convolutional layers in a convolutional neural network systematically apply learned filters to input images in order to create feature maps that summarize the presence of those features in the input.\n",
    "\n",
    "Convolutional layers prove very effective, and stacking convolutional layers in deep models allows layers close to the input to learn low-level features (e.g. lines) and layers deeper in the model to learn high-order or more abstract features, like shapes or specific objects.\n",
    "\n",
    "A limitation of the feature map output of convolutional layers is that they record the precise position of features in the input. This means that small movements in the position of the feature in the input image will result in a different feature map. This can happen with re-cropping, rotation, shifting, and other minor changes to the input image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6d2248",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. What are receptive fields and how do they work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dae230",
   "metadata": {},
   "source": [
    "The receptive field encompasses the sensory receptors that feed into sensory neurons and thus includes specific receptors on a neuron as well as collectives of receptors that are capable of activating a neuron via synaptic connections."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
