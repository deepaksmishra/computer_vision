{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "computer_vision_03.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6V5nxMeMI3H"
      },
      "source": [
        "1. After each stride-2 conv, why do we double the number of filters?\n",
        "ANs: \n",
        " Alright, let’s look back at our good old conv layers. Remember the filters, the receptive fields, the convolving? Good. Now, there are 2 main parameters that we can change to modify the behavior of each layer. After we choose the filter size, we also have to choose the stride and the padding.\n",
        "\n",
        "Stride controls how the filter convolves around the input volume. In the example we had in part 1, the filter convolves around the input volume by shifting one unit at a time. The amount by which the filter shifts is the stride. In that case, the stride was implicitly set at 1. Stride is normally set in a way so that the output volume is an integer and not a fraction. Let’s look at an example. Let’s imagine a 7 x 7 input volume, a 3 x 3 filter (Disregard the 3rd dimension for simplicity), and a stride of 1. This is the case that we’re accustomed to.\n",
        "\n",
        "2. Why do we use a larger kernel with MNIST (with simple cnn) in the first conv?\n",
        "Ans: \n",
        "Convolutional Neural Networks (CNNs) are the current state-of-art architecture for image classification task. Whether it is facial recognition, self driving cars or object detection, CNNs are being used everywhere. In this post, a simple 2-D Convolutional Neural Network (CNN) model is designed using keras with tensorflow backend for the well known MNIST digit recognition task. The whole work flow can be:\n",
        "Preparing the data\n",
        "Building and compiling of the model\n",
        "Training and evaluating the model\n",
        "Saving the model to disk for reuse\n",
        "\n",
        "\n",
        "3. What data is saved by ActivationStats for each layer?\n",
        "Ans: \n",
        "When looking at charts of activations data it can be hard to know which layer(s)' activations to look at; should I look early in the model? If there isn't enough going on in the early stages, should I look late in the model? Well, maybe the interesting stuff is actually happening in the middle...\"\n",
        "I found that for this architecture, the activations from the FeedForward layers of the transformer blocks and from the the final Linear layer seemed to be the most informative, as they changed the most as the baseline model learned. They were also the layers which changed most dramatically compared to the baseline when deliberately poor hyperparameters were used for training. \n",
        "For this example, I visualised activations from the embedding layer, final Attention Layer, and final Linear layer of the model.\n",
        "\n",
        "\n",
        "4. How do we get a learner's callback after they've completed training?\n",
        "Ans: \n",
        "Top athletes always ride the edge of over-training. The right about of training will optimize an athlete’s performance, but training too much leads to injury and reduced performance. Deep-learning models are similar. The right amount of training makes a strong model, but too much and performance can drop off on new data.\n",
        "During training deep learning models seek to minimize their loss, to be more accurate according to a given loss function. However, they judge that accuracy on the set of data they are training on. Imagine a school-kid who takes a practice test home and memorizes every problem on it and every answer. If they haven’t found the underlying patterns, the strategies that the test is meant to assess, they will be at a loss because none of the questions on the practice test are on the exam!\n",
        "Deep learning models can do that same thing. If they train too much on a dataset, they can learn that dataset specifically, rather than picking up the underlying functions connecting features and labels. The model can overfit the data.\n",
        "\n",
        "\n",
        "\n",
        "5. What are the drawbacks of activations above zero?\n",
        "Ans: It is used to determine the output of neural network like yes or no. It maps the resulting values in between 0 to 1 or -1 to 1 etc. (depending upon the function).\n",
        "The Activation Functions can be basically divided into 2 types-\n",
        "Linear Activation Function\n",
        "Non-linear Activation Functions\n",
        "\n",
        "\n",
        "6.Draw up the benefits and drawbacks of practicing in larger batches?\n",
        "Ans: \n",
        "Batch production is a method of manufacturing where identical or similar items are produced together for different sized production runs. The method allows for products to be mass-produced in batches with small to major changes to the product, from car doors through to children’s toys.\n",
        "\n",
        "Each batch goes through a single stage of the production process before moving onto the next stage to make the desired product. It’s a key solution used by businesses of various sizes across multiple industries.\n",
        "\n",
        "Changes may occur between different batches, such as products constructed in different colours, sizes, and styles. If a product needs to be altered, this variation can be changed as the production process switches from one batch style to the other.\n",
        "\n",
        "\n",
        "7. Why should we avoid starting training with a high learning rate?\n",
        "Ans: \n",
        "Ultimately, we'd like a learning rate which results is a steep decrease in the network's loss. We can observe this by performing a simple experiment where we gradually increase the learning rate after each mini batch, recording the loss at each increment. This gradual increase can be on either a linear or exponential scale.\n",
        "\n",
        "For learning rates which are too low, the loss may decrease, but at a very shallow rate. When entering the optimal learning rate zone, you'll observe a quick drop in the loss function. Increasing the learning rate further will cause an increase in the loss as the parameter updates cause the loss to \"bounce around\" and even diverge from the minima. Remember, the best learning rate is associated with the steepest drop in loss, so we're mainly interested in analyzing the slope of the plot.\n",
        "\n",
        "8. What are the pros of studying with a high rate of learning?\n",
        "Ans: \n",
        "The benefits of education are many. Not only will you personally benefit from receiving an education when it comes to income, career advancement, skill development, and employment opportunities, but your society and community receive benefits of education as well.\n",
        "\n",
        " \n",
        "\n",
        "Societies with higher rates of degree completion and levels of education tend to be healthier, have higher rates of economic stability, lower crime, and greater equality. For more surprising benefits of education, read on\n",
        "\n",
        "\n",
        "9. Why do we want to end the training with a low learning rate?\n",
        "Ans: \n",
        "The weights of a neural network cannot be calculated using an analytical method. Instead, the weights must be discovered via an empirical optimization procedure called stochastic gradient descent.\n",
        "\n",
        "The optimization problem addressed by stochastic gradient descent for neural networks is challenging and the space of solutions (sets of weights) may be comprised of many good solutions (called global optima) as well as easy to find, but low in skill solutions (called local optima).\n",
        "\n",
        "The amount of change to the model during each step of this search process, or the step size, is called the “learning rate” and provides perhaps the most important hyperparameter to tune for your neural network in order to achieve good performance on your problem.\n",
        "\n",
        "In this tutorial, you will discover the learning rate hyperparameter used when training deep learning neural networks.\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}