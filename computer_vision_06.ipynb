{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "computer_vision_06.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2mmhHJzRyXX"
      },
      "source": [
        "1. What is the difference between TRAINABLE and NON-TRAINABLE PARAMETERS?\n",
        "Ans: \n",
        "\n",
        "By default, all weights in a keras model are trainable. When you make them untrainable, the algorithm will not update these weights anymore.\n",
        "\n",
        "2. In the CNN architecture, where does the DROPOUT LAYER go?\n",
        "Ans; he original paper proposed dropout layers that were used on each of the fully connected (dense) layers before the output; it was not used on the convolutional layers.\n",
        "\n",
        "We must not use dropout layer after convolutional layer as we slide the filter over the width and height of the input image we produce a 2-dimensional activation map that gives the responses of that filter at every spatial position. So as dropout layer neutralizes (makes it zero) random neurons there are chances of loosing very important feature in an image in our training process.\n",
        "\n",
        "\n",
        "3. What is the optimal number of hidden layers to stack?\n",
        "Ans: \n",
        "One hidden layer is sufficient for the large majority of problems.\n",
        "\n",
        "\n",
        "\n",
        "4. In each layer, how many secret units or filters should there be?\n",
        "Ans: You need to use Cross-validation to test the accuracy on the test set. The optimal number of hidden units could easily be smaller than the number of inputs, there is no rule like multiply the number of inputs with N... If you have a lot of training examples, you can use multiple hidden units, but sometimes just 2 hidden units works best with little data. Usually people use one hidden layer for simple tasks, but nowadays research in deep neural network architectures show that many hidden layers can be fruitful for difficult object, handwritten character, and face recognition problems.\n",
        "\n",
        "\n",
        "\n",
        "5. What should your initial learning rate be?\n",
        "Ans: \n",
        "A traditional default value for the learning rate is 0.1 or 0.01, and this may represent a good starting point on your problem.\n",
        "\n",
        "\n",
        "6. What do you do with the activation function?\n",
        "Ans: \n",
        "Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron.\n",
        "\n",
        "\n",
        "7. What is NORMALIZATION OF DATA?\n",
        "Ans: \n",
        "Database normalization is the process of structuring a database, usually a relational database, in accordance with a series of so-called normal forms in order to reduce data redundancy and improve data integrity.\n",
        "\n",
        "\n",
        "8. What is IMAGE AUGMENTATION and how does it work?\n",
        "Ans: Image data augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset. Image data augmentation is used to expand the training dataset in order to improve the performance and ability of the model to generalize.\n",
        "\n",
        "\n",
        "\n",
        "9. What is DECLINE IN LEARNING RATE?\n",
        "Ans: \n",
        "However, examination of baseline versus retention [t(9) = 2 4.40 ... What was most interesting was the gradual slow decline in learning rate with age until .\n",
        "\n",
        "What does EARLY STOPPING CRITERIA mean?\n",
        "Ans: \n",
        "In machine learning, early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. ... Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit.\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}