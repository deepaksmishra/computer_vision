{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd8e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is the COVARIATE SHIFT Issue, and how does it affect you?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7235fee0",
   "metadata": {},
   "source": [
    "Covariate shift occurs when the distribution of variables in the training data is different to real-world or testing data. This means that the model may make the wrong predictions once it is deployed, and its accuracy will be significantly lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ce15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What is the process of BATCH NORMALIZATION?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f04f695",
   "metadata": {},
   "source": [
    "Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25641dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Using our own terms and diagrams, explain LENET ARCHITECTURE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227280a1",
   "metadata": {},
   "source": [
    "The LeNet architecture is an excellent “first architecture” for Convolutional Neural Networks (especially when trained on the MNIST dataset, an image dataset for handwritten digit recognition).\n",
    "\n",
    "LeNet is small and easy to understand — yet large enough to provide interesting results. Furthermore, the combination of LeNet + MNIST is able to run on the CPU, making it easy for beginners to take their first step in Deep Learning and Convolutional Neural Networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37395252",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Using our own terms and diagrams, explain ALEXNET ARCHITECTURE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1090721",
   "metadata": {},
   "source": [
    "AlexNet was the first convolutional network which used GPU to boost performance. 1. AlexNet architecture consists of 5 convolutional layers, 3 max-pooling layers, 2 normalization layers, 2 fully connected layers, and 1 softmax layer. ... AlexNet overall has 60 million parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c1f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Describe the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc8a02f",
   "metadata": {},
   "source": [
    "In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation. ... The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d8c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. What is NORMALIZATION OF LOCAL RESPONSE?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e36b4e",
   "metadata": {},
   "source": [
    "Local Response Normalization (LRN) was first introduced in AlexNet architecture where the activation function used was ReLU as opposed to the more common tanh and sigmoid at that time. Apart from the reason mentioned above, the reason for using LRN was to encourage lateral inhibition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98ddb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. In AlexNet, what WEIGHT REGULARIZATION was used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0985ae25",
   "metadata": {},
   "source": [
    "Use Weight Regularization to Reduce Overfitting of Deep Learning Models. Neural networks learn a set of weights that best map inputs to outputs. ... This is called weight regularization and it can be used as a general technique to reduce overfitting of the training dataset and improve the generalization of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58da0168",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Using our own terms and diagrams, explain VGGNET ARCHITECTURE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c4c4c8",
   "metadata": {},
   "source": [
    "VGG is a classical convolutional neural network architecture. It was based on an analysis of how to increase the depth of such networks. The network utilises small 3 x 3 filters. Otherwise the network is characterized by its simplicity: the only other components being pooling layers and a fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1644b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Describe VGGNET CONFIGURATIONS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f78094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. What regularization methods are used in VGGNET to prevent overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa35de1",
   "metadata": {},
   "source": [
    "adding regularization. The three most popular options are: dropout, L1 regularization and L2 regularization. In deep learning you will mostly see dropout, which I discussed earlier. Dropout deletes a random sample of the activations (makes them zero) in training. In the Vgg model this is only applied in the fully connected layers at the end of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0291ab54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
