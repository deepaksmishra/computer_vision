{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63528e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Describe the Quick R-CNN architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26eda2e",
   "metadata": {},
   "source": [
    "The Fast R-CNN consists of a CNN (usually pre-trained on the ImageNet classification task) with its final pooling layer replaced by an “ROI pooling” layer and its final FC layer is replaced by two branches — a (K + 1) category softmax layer branch and a category-specific bounding box regression branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f473f442",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Describe two Fast R-CNN loss functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8e7ff9",
   "metadata": {},
   "source": [
    "After the improvement in architecture of object detection network in R-CNN to Fast R_CNN. The training and detection time of the network decrease considerably, but the network is not fast enough to be used as a real-time system because it takes approximately (2 seconds) to generate output on an input image. The bottleneck of architecture is a selective search algorithm. Therefore K He et al. proposed a new architecture called Faster R-CNN. It does not use selective search instead they propose another region proposal generation algorithm called Region Proposal Network. Let’s discuss the Faster R-CNN architecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87b8981",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Describe the DISABILITIES OF FAST R-CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bd0358",
   "metadata": {},
   "source": [
    "\n",
    "In Fast R-CNN, the image is fed to the underlying CNN just once and the selective search is run on the other hand as usual. These region proposals generated by Selective Search are then projected on to the feature maps generated by the CNN. This process is called ROI Projection(Region Of Interest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb506fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Describe how the area proposal network works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c657720",
   "metadata": {},
   "source": [
    "A Region Proposal Network, or RPN, is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361e8c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Describe how the RoI pooling layer works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bcfb5b",
   "metadata": {},
   "source": [
    "ROI pooling solves the problem of fixed image size requirement for object detection network. RO I pooling produces the fixed-size feature maps from non-uniform inputs by doing max-pooling on the inputs. The number of output channels is equal to the number of input channels for this layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d622ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. What are fully convolutional networks and how do they work? (FCNs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e8dd41",
   "metadata": {},
   "source": [
    "One way to counter the drawbacks of the previous Architecture is by stacking a number of Convolution Layers having similar padding to preserve dimension and output a final segmentation map. Meaning the model will learn the mapping from the input image to its corresponding segmentation map through the successive transformation of feature mappings. Seems good, but there is a major issue with this Architecture as well. We use the same padding in all the Convolution layers because we would want the output image to be of the same dimension as the input image. But this preservation of full-resolution becomes quite computationally expensive.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f6b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. What are anchor boxes and how do you use them?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19341b5d",
   "metadata": {},
   "source": [
    "When you use a neural network like YOLO or SDD to predict multiple objects in a picture, the network is actually making thousands of predictions and only showing the ones that it decided were an object.\n",
    "Prediction 1: (X, Y, Height, Width), Class\n",
    "….\n",
    "Prediction ~80,000: (X, Y, Height, Width), Class\n",
    "Where the(X, Y, Height, Width) is called the “bounding box”, or box surrounding the objects. This box and the object class are labelled manually by human annotators.\n",
    "In an extremely simplified example, imagine that we have a model that has two predictions and receives the following image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af368843",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Describe the Single-shot Detector&#39;s architecture (SSD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d8860c",
   "metadata": {},
   "source": [
    "Instead of using sliding window, SSD divides the image using a grid and have each grid cell be responsible for detecting objects in that region of the image. Detection objects simply means predicting the class and location of an object within that region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8debcd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. HOW DOES THE SSD NETWORK PREDICT?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a042f13",
   "metadata": {},
   "source": [
    "SSD uses a matching phase while training, to match the appropriate anchor box with the bounding boxes of each ground truth object within an image. Essentially, the anchor box with the highest degree of overlap with an object is responsible for predicting that object's class and its location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70308c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. Explain Multi Scale Detections?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fa3f1a",
   "metadata": {},
   "source": [
    "A unified deep neural network, denoted the multi-scale CNN (MS-CNN), is proposed for fast multi-scale object detection. ... In the proposal sub-network, detection is performed at multiple output layers, so that receptive fields match objects of different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f0f4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "11. What are dilated (or atrous) convolutions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386b8b0a",
   "metadata": {},
   "source": [
    "Dilated convolutions, also known as atrous convolutions, have been widely explored in deep convolutional neural networks (DCNNs) for various tasks like semantic image segmentation, object detection, audio generation, video modeling, and machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef62374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20854f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
